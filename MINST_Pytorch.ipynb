{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class Config:\n",
        "    batch_size = 64\n",
        "    epoch = 10\n",
        "    momentum = 0.9\n",
        "    alpha = 1e-3\n",
        "\n",
        "    print_per_step = 100\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(64 * 5 * 5, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),# Method to speed up convergence\n",
        "            # (Batch normalization is generally placed behind the fully connected layer and in front of the activation function layer)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TrainProcess:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train, self.test = self.load_data()\n",
        "        self.net = LeNet()\n",
        "        self.criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "        self.optimizer = optim.SGD(self.net.parameters(), lr=Config.alpha, momentum=Config.momentum)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data():\n",
        "        print(\"Loading Data......\")\n",
        "        \"\"\"Load the MNIST dataset, if the local data does not exist, it will be downloaded automatically\"\"\"\n",
        "        train_data = datasets.MNIST(root='./data/',\n",
        "                                    train=True,\n",
        "                                    transform=transforms.ToTensor(),\n",
        "                                    download=True)\n",
        "\n",
        "        test_data = datasets.MNIST(root='./data/',\n",
        "                                   train=False,\n",
        "                                   transform=transforms.ToTensor())\n",
        "\n",
        "        # returns a data iterator\n",
        "        # shuffleï¼šWhether to shuffle the order\n",
        "        train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                                   batch_size=Config.batch_size,\n",
        "                                                   shuffle=True)\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                                  batch_size=Config.batch_size,\n",
        "                                                  shuffle=False)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    def train_step(self):\n",
        "        steps = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        print(\"Training & Evaluating......\")\n",
        "        for epoch in range(Config.epoch):\n",
        "            print(\"Epoch {:3}\".format(epoch + 1))\n",
        "\n",
        "            for data, label in self.train:\n",
        "                data, label = Variable(data.cpu()), Variable(label.cpu())\n",
        "                self.optimizer.zero_grad()  # zero the gradient\n",
        "                outputs = self.net(data)  # Pass the data into the network for forward calculation\n",
        "                loss = self.criterion(outputs, label)  # get the loss function\n",
        "                loss.backward()  # backpropagation\n",
        "                self.optimizer.step()  # One-step parameter update via gradient\n",
        "\n",
        "                # Print the result every 100 times\n",
        "                if steps % Config.print_per_step == 0:\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    correct = int(sum(predicted == label))\n",
        "                    accuracy = correct / Config.batch_size  # Calculation accuracy\n",
        "                    end_time = datetime.now()\n",
        "                    time_diff = (end_time - start_time).seconds\n",
        "                    time_usage = '{:3}m{:3}s'.format(int(time_diff / 60), time_diff % 60)\n",
        "                    msg = \"Step {:5}, Loss:{:6.2f}, Accuracy:{:8.2%}, Time usage:{:9}.\"\n",
        "                    print(msg.format(steps, loss, accuracy, time_usage))\n",
        "\n",
        "                steps += 1\n",
        "\n",
        "        test_loss = 0.\n",
        "        test_correct = 0\n",
        "        for data, label in self.test:\n",
        "            data, label = Variable(data.cpu()), Variable(label.cpu())\n",
        "            outputs = self.net(data)\n",
        "            loss = self.criterion(outputs, label)\n",
        "            test_loss += loss * Config.batch_size\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct = int(sum(predicted == label))\n",
        "            test_correct += correct\n",
        "\n",
        "        accuracy = test_correct / len(self.test.dataset)\n",
        "        loss = test_loss / len(self.test.dataset)\n",
        "        print(\"Test Loss: {:5.2f}, Accuracy: {:6.2%}\".format(loss, accuracy))\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        time_diff = (end_time - start_time).seconds\n",
        "        print(\"Time Usage: {:5.2f} mins.\".format(time_diff / 60.))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = TrainProcess()\n",
        "    p.train_step()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}